{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib as hl\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "directory = \"C://Users//RAGS//Desktop//Dev_Env//test_notes\"\n",
    "out_file = \"errors.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_file(filename):\n",
    "   \"\"\"\"This function returns the SHA-1 hash\n",
    "   of the file passed into it\"\"\"\n",
    "\n",
    "   # make a hash object\n",
    "   h = hl.sha1()\n",
    "\n",
    "   # open file for reading in binary mode\n",
    "   with open(filename,'rb') as file:\n",
    "\n",
    "       # loop till the end of the file\n",
    "       chunk = 0\n",
    "       while chunk != b'':\n",
    "           # read only 1024 bytes at a time\n",
    "           chunk = file.read(1024)\n",
    "           h.update(chunk)\n",
    "\n",
    "   # return the hex representation of digest\n",
    "   return h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir_path):\n",
    "    file_paths = []\n",
    "    for root, directories, files in os.walk(dir_path):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_file(filename):\n",
    "    files = list_files(directory)\n",
    "    hash = []\n",
    "    for i in files:\n",
    "        hash.append(hash_file(i))\n",
    "    #hash\n",
    "    df = pd.DataFrame(files, columns=['filenames'])\n",
    "    df['hash'] = hash\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import google.auth\n",
    "from googleapiclient.http import MediaFileUpload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "def create_service():\n",
    "    \"\"\"Shows basic usage of the Drive v3 API.\n",
    "    Prints the names and ids of the first 10 files the user has access to.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "        \n",
    "    # create drive api client\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(service, filename):\n",
    "    #service = create_service()\n",
    "    try:\n",
    "\n",
    "        file_metadata = {'name': filename}\n",
    "        media = MediaFileUpload(filename)\n",
    "        # pylint: disable=maybe-no-member\n",
    "        file = service.files().create(body=file_metadata, media_body=media,\n",
    "                                      fields='id').execute()\n",
    "        print(F'File ID: {file.get(\"id\")}')\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(F'An error occurred: {error}')\n",
    "        file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_files_in_drive(service):\n",
    "    #service = create_service()\n",
    "    try:\n",
    "        # Call the Drive v3 API\n",
    "        results = service.files().list(\n",
    "            pageSize=10, fields=\"nextPageToken, files(id, name)\").execute()\n",
    "        items = results.get('files', [])\n",
    "\n",
    "        if not items:\n",
    "            print('No files found.')\n",
    "            return\n",
    "        print('Files:')\n",
    "        for item in items:\n",
    "            print(u'{0} ({1})'.format(item['name'], item['id']))\n",
    "    except HttpError as error:\n",
    "        # TODO(developer) - Handle errors from drive API.\n",
    "        print(f'An error occurred: {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(service, folder_name):\n",
    "    \"\"\" Create a folder and prints the folder ID\n",
    "    Returns : Folder Id \"\"\"\n",
    "\n",
    "    # create drive api client\n",
    "    #service = create_service()\n",
    "    try:\n",
    "        file_metadata = {\n",
    "            'name': folder_name,\n",
    "            'mimeType': 'application/vnd.google-apps.folder'\n",
    "        }\n",
    "\n",
    "        # pylint: disable=maybe-no-member\n",
    "        file = service.files().create(body=file_metadata, fields='id').execute()\n",
    "                                      \n",
    "        print(F'Folder ID: \"{file.get(\"id\")}\".')\n",
    "        return file.get('id')\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(F'An error occurred: {error}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_folder(service, folder_id, filename):\n",
    "    \"\"\"Upload a file to the specified folder and prints file ID, folder ID\n",
    "    Args: Id of the folder\n",
    "    Returns: ID of the file uploaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_metadata = {\n",
    "            'name': filename,\n",
    "            'parents': [folder_id]\n",
    "        }\n",
    "        media = MediaFileUpload(filename, resumable=True)\n",
    "        # pylint: disable=maybe-no-member\n",
    "        file = service.files().create(body=file_metadata, media_body=media,\n",
    "                                      fields='id').execute()\n",
    "        print(F'File ID: \"{file.get(\"id\")}\".')\n",
    "        return file.get('id')\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(F'An error occurred: {error}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hash_file('old_hash.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hash_file('new_hash.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('old_hash.csv')\n",
    "df2 = pd.read_csv('new_hash.csv')\n",
    "df=pd.concat([df1,df2])\n",
    "df.drop(0,axis=1,inplace=True)\n",
    "df\n",
    "#df.to_excel('final.xlsx')\n",
    "# df.drop_duplicates(keep=False, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = create_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_files_in_drive(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=list_files(directory)\n",
    "for file in files:\n",
    "    upload_file(service, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This message will be displayed on the screen.')\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "\n",
    "with open(out_file, 'w') as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.\n",
    "    print('This message will be written to a file.')\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559ace3a43580f24d3b0afb83825b48f66df98e265f78a71c1931525a081c6dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
